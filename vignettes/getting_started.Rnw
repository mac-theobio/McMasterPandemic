%\VignetteIndexEntry{getting started}
%\VignetteEngine{knitr::knitr}
%\VignetteEncoding{UTF-8}
\documentclass[12pt]{article}
\input{nlipreamble}

%% code listing
\usepackage{listings}
\usepackage{color}
\lstset{
    showstringspaces=false,
    basicstyle=\ttfamily,
    commentstyle=\color[grey]{0.6},
    stringstyle=\color[RGB]{255,150,75}
}
\newcommand{\inlinecode}[2]{{\lstinline[language=#1]$#2$}}
\renewcommand{\code}[1]{\inlinecode{R}{#1}}

<<setup,include=FALSE>>=
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.height = 4,
  dev = "pdf",
  ##dev = "tikz",
  cache = FALSE,
  error = FALSE,  ## override knitr::knit() default
  ## error=TRUE,  ## allow run to continue with error (good for debugging1)
  ##warning=FALSE,
  comment = "#>"
)
@

\title{\texttt{McMasterPandemic}: getting started}

\author{Ben Bolker and David Earn\\\texttt{earn@math.mcmaster.ca}}

\begin{document}
\linenumbers

\maketitle

\begin{abstract}
  \texttt{McMasterPandemic} is an R package that provides tools
  for simulating and forecasting infectious disease outbreaks, using
  compartmental epidemic models.  The primary mechanistic framework is
  a susceptible-exposed-infectious-removed (SEIR) model, with
  additional compartments for individuals in acute and intensive care
  units in hospitals.
\end{abstract}

\tableofcontents

\section{Installation}

Use \code{remotes::install_github("bbolker/McMasterPandemic")} to install the latest version of the package.

<<loadpkg, message=FALSE>>=
library(McMasterPandemic)
@

In this vignette we'll also use some other packages:

<<loadpkgs,message=FALSE>>=
library(ggplot2); theme_set(theme_bw())
library(cowplot)
@

\section{Data requirements}

\paragraph{Parameters}

To run simulations, a few parameter values must be specified.
Set these by editing the example params file, which is converted
to a \code{params_pansim} object by \code{read_params()}.
In the example, the time unit is assumed to be days.

The term ``in acute care'' means ``in hospital but not in the
intensive care unit (ICU)''.

<<set_params>>=
params1 <- read_params("ICU1.csv")
@
\noindent
(by default \code{read_params} looks first in the working directory
for CSV files, then in the \code{params} directory installed with the
package (\code{system.file("params", package="McMasterPandemic")}).
All the built-in parameter files can be found as follows:

<<params_files>>=
folder <- system.file("params", package="McMasterPandemic")
list.files(folder)
@ 
\noindent
If you want to edit one of these files, you need to copy it to your
working directory first.  To find the full path to \texttt{ICU1.csv},
for example, use:
<<find_full_path, tidy=FALSE>>=
system.file("params/ICU1.csv", package="McMasterPandemic")
@ 

If \code{p} is a parameter set (e.g., the result of
\code{read_params}), then \code{print(p, describe=TRUE)} or,
equivalently, \code{describe_params(p)} will return a data frame with
a column giving the meaning of each parameter.
<<params_kable>>=
knitr::kable(describe_params(params1))
@

\bigskip

The \code{summary} method for \code{params_pansim} objects returns the
initial exponential growth rate ($r_0$), the doubling time
($\log{2}/r_0$), the mean generation interval ($\overline{G}$), and
the basic reproduction number

%%
\begin{equation*}
\R_0 = \beta_0 \left\{
\alpha \frac{C_{\rm a}}{\gamma_{\rm a}}
+
(1-\alpha)\left[ \frac{C_{\rm p}}{\gamma_{\rm p}}
  + \mu(1-\texttt{iso}_{\rm m})\frac{C_{\rm m}}{\gamma_{\rm m}}
  + (1-\mu)(1-\texttt{iso}_{\rm s})\frac{C_{\rm s}}{\gamma_{\rm s}} \right]
  \right\} \,.
\end{equation*}

<<params_summary>>=
knitr::kable(round(t(summary(params1)),2))
@

\bigskip\noindent
The components of $\R_0$ (the reproduction number associated with each infectious compartment) can also be extracted.

<<R0_components>>=
knitr::kable(round(t(get_R0(params1, components=TRUE)),2))
@

\paragraph{Initial conditions}

The initial state must also be set, but it is sufficient to specify
the parameter set (a \code{params_pansim} object), in which case
the population size and initially exposed population will be taken from the parameters (in this case all non-exposed individuals
are assumed to be susceptible).

<<set_initial_state>>=
state1 <- make_state(params=params1)
@

\paragraph{Start and end dates}

Dates on which the simulation starts and ends must be stated.  If
there are no observations that you are aiming to match, then these
dates are arbitrary and only the length of time matters.

<<dates>>=
sdate <- "2020-Feb-10"
edate <- "2020-Jun-1"
@

\section{Running a simulation}\label{sec:run_sim}

A simple deterministic simulation is run as follows, and returns a
\code{pansim} object.  The \code{summary} method computes the times
and magnitudes of peak demands on acute care (H) and intenstive care
(ICU), and the basic reproduction number $\R_0$.

<<run_sim>>=
res1 <- run_sim(params=params1, state=state1, start_date=sdate, end_date=edate)
summary(res1)
@

The \code{plot} method for \code{pansim} objects returns a
\code{ggplot} object, optionally on a log scale.

<<plot_sim,fig.width=8>>=
plot_grid(plot(res1, log=TRUE),  ## logarithmic
          plot(res1))  ## linear
@

\subsection{Stochasticity}

The effects of observation error are easy to
explore with the \code{stoch} argument to \code{run_sim}.
The \code{obs_disp} parameter is the dispersion parameter for a negative
binomial.

<<run_sim_obs_noise,fig.width=8>>=
set.seed(101)
params1obs <- update(params1, obs_disp=200)
res1obs <- run_sim(params1obs, state1, start_date=sdate, end_date=edate,
                   stoch=c(obs=TRUE, proc=FALSE))
summary(res1obs)
plot_grid(plot(res1obs, log=TRUE),
          plot(res1obs))
@

To simulate with process error, use \code{stoch=c(..., proc=TRUE)}. By default, this simulates only demographic stochasticity, which has little effect in a large epidemic.

<<proc1>>=
params1proc <- update(params1,E0=200,proc_disp=0)  ## demog stoch only
res1proc <- run_sim(params1proc, start_date=sdate, end_date=edate,
                    stoch=c(obs=FALSE, proc=TRUE))
@

Making \code{proc_disp} positive simulates with additional process noise:

<<proc2>>=
params1proc2 <- update(params1,E0=200, proc_disp=0.5, obs_disp=5)
res1proc2 <- run_sim(params1proc2, start_date=sdate, end_date=edate,
                    stoch=c(obs=FALSE, proc=TRUE))
plot_grid(plot(res1proc2, log=TRUE), plot(res1proc2))
@

\paragraph{Technical note.}

Demographic noise is included by calculating probabilities from the
rates and then drawing a multinomial sample to determine how many
individuals move from one compartment to each of the others.  With
pure demographic noise, the CV is very small with only $\sim1000$
individuals moving among compartments.  Process dispersion
(\code{proc_disp}; ``overdispersed demographic stochasticity'') is
implemented using \code{pomp::reulermultinom}, which adds gamma white
noise to the event rates.  For some discussion of this, see p.\,274
and Appendix~A of the ``plug-and-play'' paper by He \emph{et al.}
(2010, \emph{J.\ R.\ Soc.\ Interface} {\bfseries7}, 271--283,
doi:\texttt{10.1098/rsif.2009.0151}.  \david{The intensity of the
  gamma white noise process (\code{proc_disp}) has units
  (\emph{cf.}~$\sigma_{\rm SE}$ in He \emph{et al.}); it would be
  easier to think about the cofficient of variation (CV) rather than
  standard deviation (sd).}

\david{Notes scribbled from discussion with BB: To get CIs on a
  forecast, we could hack by adjusting \code{proc_disp} until getting
  CIs that are plausibly wide; estimating this number is a can of
  worms.  A slighty more principaled way to decide on that number: fit
  params, then run sims with different combinations of obs and proc
  noise that yield noise like in the data: then infer how observed
  noise is divided btw proc and measurement error.}

\david{DC commented on 19 Apr 2020 (`MP updates' thread): ``5/ I have
  had the same question for a while regarding noise amplitude… I
  usually look at the variance of the data as a guidance, but never
  did anything formal.  6/ I often find myself starting with MCMC,
  just to give it up for ABC or something else a few days/weeks down
  the road because I end up spending way too much time in trying to
  fix more or less technical issues regarding convergence (I use Stan
  nearly all the time, maybe that’s why…).''}

\subsection{Time-dependent transmission rate}

Implementing known changes in transmission rate (e.g., resulting from
social distancing measures) is straighforward via the \code{time_pars}
argument.  The following reduces $\beta_0$ (and hence $\R_0$) to 50\%
of its original value on 10 March 2020, and to 10\% of its original
value on 25 March 2020.

Setting \code{ndt=20} forces 20 intermediate time steps to occur
between each saved step.  (Try it with \code{ndt=1} to see why this is
a good idea.)

Setting \code{condense=FALSE} retains all variables in the output,
rather than collapsing into a single $I$ class \emph{etc.}

<<run_sim_time-dependent_beta0,fig.width=8>>=
time_pars <- data.frame(Date=c("2020-Mar-10","2020-Mar-25"),
                      Symbol=c("beta0","beta0"),
                      Relative_value=c(0.5,0.1))
restimedep <- run_sim(params1,state1,start_date=sdate,end_date=edate,
                      params_timevar=time_pars,ndt=20, condense=FALSE)
summary(restimedep)
plot_grid(plot(restimedep, log=TRUE, condense=FALSE),
          plot(restimedep, condense=FALSE))
@

\section{Changing parameters}

Some parameters you might wish to change are not directly available in
the parameter file.  Instead, you can adjust them using
\code{fix_pars()}.  For example, if you would like to change the
default value of $\R_0$ implied in the parameter list \code{params1}
you can do the following.

<<changeR0>>=
print(summary(params1))
## Change R0 to 2
newparams1 <- fix_pars(params1, target=c(R0=2)) 
print(summary(newparams1))
@ 

\david{See \texttt{refactor.Rmd} for functions not yet described here.}

\section{Calibration}

In a typical epidemic forecasting application, we have imperfect
information about the parameters and a time series of reported events
(e.g., cases, hospitalizations, deaths, \emph{etc.}).  Our goal is to
predict the future course of the outbreak, and to determine how it
will differ under various intervention scenarios.

The natural approach is to find a set of parameters that lies within
the estimated constraints and best fits the observed part of the
epidemic.  This is referred to as ``calibrating'' the model to the data.

Unsurprisingly, there is a function \code{calibrate()} for doing just this.

Imagine that the simulated data saved in \code{res1obs} were the
observed data to which want to fit the model.  We can calibrate to
these data as follows.

Note that \code{calibrate()} requires the data come in ``long form'', which means that for each date on which we have data, there are separate rows for each type of data (report, death, hospitalization, \emph{etc}).  This is in contrast to ``wide form'', for which there is one row for each date, and separate columns for each observed variable.

\david{I am not supressing warnings so users aren't alarmed when they see these warnings themselves.  We should, of course, consider revisions that either avoid warnings or explain clearly to the user how to resolve them.}

<<calibrateTOres1obs, message=FALSE>>=
library(dplyr)
## pull out only the reported cases and convert to long form:
report_data <- (res1obs 
    %>% mutate(value=round(report), var="report")
    %>% select(date, value, var)
    %>% na.omit()
)
head(report_data)
## beta0 is the only parameter we're going to optimize:
opt_pars <- list(params = c(beta0=0.1))
## fit beta0 based on the report data:
fitted.mod <- calibrate(
    data = report_data
  , start_date = sdate
    ## skip breaks that are present by default:
  , time_args = list(break_dates = NULL)
  , base_params = params1obs
  , opt_pars = opt_pars
  ##, debug_plot = TRUE # instructive plotting during optimization
)
## plot the resulting fit
plot(fitted.mod, data=report_data)
## spit out fitted parameters (in this case, just beta0)
coef(fitted.mod, "fitted")
@ 
\noindent
That worked well, given that the value of \code{beta0} used
for the simulation was 1.
You might want to try running the above interactive
without commenting out ``\code{debug_plot = TRUE}''.
This will allow you to see the process of fitting the
model to the data. Note, however, that this instructive
visualization of the optimization process will slow down
the optimization by an order of magnitude.

Let's now now try to fit the model to both reports and deaths.
It is easiest to create the required long-form data frame
using the \code{pivot_longer} function in the \code{tidyr} package.

<<make_long_form_reports_and_deaths, message=FALSE>>=
library(tidyr)
report_death_data <- (res1obs 
    %>% select(date, report, death)
    %>% pivot_longer(names_to = "var", -date)
    %>% mutate(value=round(value))
    %>% na.omit()
)
head(report_death_data, n=12)
@ 

Now let's fit to both reports and deaths.

<<calibrateTOres1obs_both_reports_and_deaths, message=FALSE>>=
## beta0 is the only parameter we're going to optimize:
opt_pars <- list(params = c(beta0=0.1))
fitted.mod <- calibrate(
    data = report_death_data
  , start_date = sdate
    ## skip breaks that are present by default:
  , time_args = list(break_dates = NULL)
  , base_params = params1obs
  , opt_pars = opt_pars
  ##, debug_plot = TRUE # instructive plotting during optimization
)
plot(fitted.mod, data=report_death_data)
@ 
\noindent
If you wish, you can plot just the data being fitted, and the fitted
model, via:
<<plot_report_and_death_only>>=
plot(fitted.mod, data=report_death_data, 
     predict_args=list(keep_vars=c("report","death")))
@ 
\noindent
That fit looks remarkably good.  Let's see how good:
<<fitted_mod_summary>>=
coef(fitted.mod, "fitted") # spit out fitted parameters
summary(coef(fitted.mod))
@ 
\noindent
Amazing: our fitted \code{beta0} is exactly the value used
in the simulation that generated the data.
Note that in the summary at the end, \code{r0} refers to the
initial exponential growth rate from the fitted model.
This provides an alternative to the \code{epigrowthfit}
package for fitting epidemic growth rates.

Finally, consider the case where we have both observation and process noise.  Fitting to these data won't do as well, because \code{calibrate()} does not have a way of fitting to process noise.  Consequently, the quality of our fit can be expected to be worse.  Of course, real data always contain process noise\dots

<<calibrateTOres1proc2>>=
report_data <- (res1proc2 
    %>% mutate(value=round(report), var="report")
    %>% select(date, value, var)
    %>% na.omit()
)
## beta0 is the only parameter we're going to optimize:
opt_pars <- list(params = c(beta0=0.1))
fitted.mod <- calibrate(
    data = report_data
  , start_date = sdate
    ## skip breaks that are present by default:
  , time_args = list(break_dates = NULL)
  , base_params = params1proc2
  , opt_pars = opt_pars
  ##, debug_plot = TRUE # instructive plotting during optimization
)
plot(fitted.mod, data=report_data)
coef(fitted.mod, "fitted") # spit out fitted parameters
summary(coef(fitted.mod,"all"))
@ 
\noindent
As above, you can plot just the data being fitted, and the fitted
model, via:
<<plot_report_only>>=
plot(fitted.mod, data=report_data, predict_args=list(keep_vars="report"))
@ 

\subsection{Troubleshooting calibrations}

If you find that the fitted model trajectory is peculiarly jagged, the
likely culprit is the time step.  In this case, increase the number of
internal time steps per time step (\code{ndt}), via adding
\code{sim_args} to your \code{calibrate()} call as follows:
<<troubleshooting_ndt, eval=FALSE>>=
    , sim_args = list(ndt = 2)
@ 
\noindent
You may need to experiment with \code{ndt} to get a smooth result.

\section{Scenario exploration}

Typically, after calibrating to observed data, you are likely to be
interested in forecasting what might happen in the future, under
various scenarios of possible changes in control measures/policies.
Here, we give an example involving changing the transmission rate
($\beta$) in the future.

First we load some data manipulation packages for convenience.
<<time_varying_bt_using_run_sim, message=FALSE>>=
library(zoo)
library(tidyverse)
@ 
\noindent
Now we modify the \code{run_sim} example (\cref{sec:run_sim}).  We
first check that setting \code{Relative_value=1} and using
non-\code{timevar} \code{run_sim} yield the same results.
<<modified_example_from_run_sim>>=
params <- read_params("ICU1.csv")
pp <- fix_pars(params, target = c(R0 = 1.3, Gbar=6))
state <- make_state(params=pp)
startdate <- as.Date("2020-01-01")
enddate <- as.Date("2020-10-01")
@ 
\noindent
This is checking if we can get the same thing if we don't add stoch:
<<check_get_same_thing>>=
sim0 <- run_sim(pp,state,start_date=startdate,end_date=enddate)
gg0 <- (ggplot(sim0,aes(x=date))
	+ geom_point(aes(y=incidence))
)
print(gg0)
@ 
\noindent
We want a dataframe that includes the time varying relative $\beta$ at
each saved time point.  If relative $\beta$ is constant though time,
it should give back the same trajectory.
<<set_time_pars>>=
time_pars <- data.frame(Date=as.Date(startdate:enddate)
	, Symbol="beta0"
	, Relative_value=1
)
	# , stringsAsFactors=FALSE)
@ 
\noindent
This fits a \code{timevar} dataframe where \code{beta0=1}:
<<>>=
sim0_t <- update(sim0, params_timevar=time_pars)
print(gg0
	+ geom_point(data=sim0_t, aes(x=date,y=incidence), color="red")
)
@ 
\noindent
Now, as an example, we set relative $\beta$ to drop by a factor of 2
(linearly) between 1 July 2020 and 1 Oct 2020.
<<>>=
lockdown <- as.Date("2020-07-01")
time_pars2 <- 
    data.frame(Date=as.Date(startdate:enddate)
             , Symbol="beta0"
             , Relative_value = 
                   c(rep(1, length(startdate:lockdown)-1)
                   , seq(1,0.5,length.out = length(lockdown:enddate))
                     )
               )
##print(time_pars2)
head(time_pars2)
@ 

<<>>=
sim0_t_reduce <- update(sim0, params_timevar=time_pars2)
gg_rel_beta <- (ggplot(time_pars, aes(x=Date))
	+ geom_point(aes(y=Relative_value))
	+ geom_point(data=time_pars2, aes(x=Date, y=Relative_value), color="red")
)
@ 
\noindent
We can now look at the relative value of $\beta$ in each scenario, and the corresponding forecasted epidemic curves.
<<>>=
print(gg_rel_beta)
print(gg0
		+ geom_point(data=sim0_t_reduce, aes(x=date,y=incidence), color="red")
)
@ 

\end{document}
