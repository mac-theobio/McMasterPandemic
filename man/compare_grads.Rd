% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tmbutils.R
\name{compare_grads}
\alias{compare_grads}
\alias{compare_hessians}
\title{Compare Automatic and Numerical Differentiation}
\usage{
compare_grads(model, tolerance = 1e-05, tmb_pars = NULL, ...)

compare_hessians(model, tolerance = 1e-05, tmb_pars = NULL, ...)
}
\arguments{
\item{model}{flexmodel}

\item{tolerance}{numerical tolerance to pass to \code{all.equal} --
if \code{NA}, then a list is returned that can be passed to
\code{all.equal} using \code{do.call}}

\item{tmb_pars}{optional parameters to pass to the tmb functions}

\item{...}{additional arguments to pass to \code{numDeriv::grad}}
}
\value{
either (1) the return value of \code{all.equal} comparing
gradients using TMB auto differentiation to \code{numDeriv::grad} if
\code{is.numeric(tolerance)},
or (2) a list is returned that can be passed to
\code{all.equal} using \code{do.call} if \code{is.na(tolerance)}
}
\description{
Compare differences in gradients of loss functions computed using
automatic and numerical differentiation
}
\seealso{
Other compare: 
\code{\link{compare_sims}()},
\code{\link{time_wrap}()}
}
\concept{compare}
